{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fcfdc8",
   "metadata": {},
   "source": [
    "#### Softmax and Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33209b",
   "metadata": {},
   "source": [
    "Softmax is a function that distributes a series of values from 0 to 1 where the sum of all values in the distribution will sum to 1. This is particularly useful in the case of multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c131a4c0",
   "metadata": {},
   "source": [
    "$S(y_i) = \\frac{e^{y_i}}{\\sum_{all y_j} {e^{y_j}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b172e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11910ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b43def97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0320586  0.08714432 0.23688282 0.64391426]\n"
     ]
    }
   ],
   "source": [
    "# Test softmax function\n",
    "arr1 = np.array([1, 2, 3, 4])\n",
    "softarr1 = softmax(arr1)\n",
    "print(softarr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058b120",
   "metadata": {},
   "source": [
    "Cross Entropy loss is a loss function that measures how far the predicted labels diverge or deviate from the actual labels. This is also useful for multi-class classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393eb77",
   "metadata": {},
   "source": [
    "$D(\\hat{Y}, Y) = -\\frac{1}{N}\\sum{Y_i log(\\hat{Y_i})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0da5b",
   "metadata": {},
   "source": [
    "It is to be noted that the actual predictions Y must be one-hot encoded (true class = 1, other classes = 0; e.g. [1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99288830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy loss function\n",
    "def crossentropy(y, yhat):\n",
    "    return -np.sum(y * np.log(yhat)) / float(yhat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32993754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for arr2good: 0.11889164797957748\n",
      "Loss for arr2bad: 0.23104906018664842\n"
     ]
    }
   ],
   "source": [
    "arr2 = np.array([1, 0, 0])\n",
    "arr2good = np.array([0.7, 0.2, 0.1])\n",
    "arr2bad = np.array([0.5, 0.3, 0.2])\n",
    "arr2goodcross = crossentropy(arr2, arr2good)\n",
    "arr2badcross = crossentropy(arr2, arr2bad)\n",
    "print(f\"Loss for arr2good: {arr2goodcross}\")\n",
    "print(f\"Loss for arr2bad: {arr2badcross}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6825dd1",
   "metadata": {},
   "source": [
    "In PyTorch, these functions are built into the nn library as the nn.CrossEntropyLoss() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e10c0",
   "metadata": {},
   "source": [
    "It is to be noted that when using the nn.CrossEntropyLoss() function:\n",
    "<ul>\n",
    "    <li>It implements both the softmax and the negative log likelihood functions</li>\n",
    "    <li>Y has class labels and is <b>NOT one-hot encoded</b></li>\n",
    "    <li>Y_predictions has raw scores for the classes</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c902a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5951)\n",
      "tensor(1.4411)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch example\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Actual class\n",
    "y = torch.tensor([0]) # True class is class 0\n",
    "yhat1 = torch.tensor([[1.0, 0.1, 0.1]]) # Predicting strongly class 0\n",
    "yhat2 = torch.tensor([[0.1, 0.1, 0.9]]) # Predicting strongly class 2\n",
    "\n",
    "l1 = loss(yhat1, y) # Low\n",
    "l2 = loss(yhat2, y) # High\n",
    "print(l1)\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c1e8209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Combine with torch.max() to get the class label\n",
    "print(torch.max(yhat1, 1)[1].item())\n",
    "print(torch.max(yhat2, 1)[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b41b446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5951)\n",
      "tensor(1.1745)\n"
     ]
    }
   ],
   "source": [
    "# To do this across multiple samples\n",
    "# Actual classes\n",
    "y = torch.tensor([0, 1, 0]) # True classes are class 0, 1 and 0 respectively\n",
    "yhat1 = torch.tensor([[1.0, 0.1, 0.1],\n",
    "                     [0.1, 1, 0.1],\n",
    "                     [1.0, 0.1, 0.1]]) # Predicting strongly class 0, 1 and 0 respectively\n",
    "yhat2 = torch.tensor([[0.1, 0.1, 0.9],\n",
    "                     [0.9, 0.1, 0.1],\n",
    "                     [0.9, 0.1, 0.1]]) # Predicting strongly class 2, 0, and 0 respectively\n",
    "\n",
    "l1 = loss(yhat1, y) # Low\n",
    "l2 = loss(yhat2, y) # High\n",
    "print(l1)\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bfec724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0])\n",
      "tensor([2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Combine with torch.max() to get the class label\n",
    "print(torch.max(yhat1, 1)[1])\n",
    "print(torch.max(yhat2, 1)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

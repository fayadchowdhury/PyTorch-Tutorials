{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa7adba",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c159aaf",
   "metadata": {},
   "source": [
    "A convolutional neural network (CNN) is different from a regular neural network in that it applies filters or filter kernels and they work best on image data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6806c55",
   "metadata": {},
   "source": [
    "In this notebook, we are going to apply a CNN to the CIFAR10 dataset, which is an image dataset with 10 classes each with 6000 32x32 coloured images (60000 images total). This task is an image multiclass classification task. The dataset was published by the Canadian Institution for Advanced Research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778d13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f8e997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Configure device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b467538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 4\n",
    "batch_size = 4\n",
    "alpha = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "746b47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite transform to convert CIFAR10 dataset\n",
    "# (PIL (Python Imaging Library) images of range[0, 1]) \n",
    "# into normalized tensors of range[-1, 1]\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize R, G and B channels with mean 0.5 and standard deviation 0.5\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f824ce6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms) # Download training version of CIFAR10 data applying the transforms\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms) # Download testing version of CIFAR10 data applying the transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3a53f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into DataLoader\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c3fccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f6b37",
   "metadata": {},
   "source": [
    "The output of a convolutional layer is as follows:<br>\n",
    "Given a square image of IxI pixels, a filter kernel of size FxF, P padding and S strides:<br>\n",
    "Output is OxO matrix where:<br>\n",
    "$O = \\frac{I-F+2P}{S} + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ae575",
   "metadata": {},
   "source": [
    "The output of a pooling layer is as follows:<br>\n",
    "Given a square image of IxI pixels, a filter kernel of size FxF and S strides:<br>\n",
    "Output is OxO matrix where:<br>\n",
    "$O = \\frac{I-F}{S} + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5da3fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self): # Kinda hard to figure out the parameters beforehand, better to go layer by layer within the __init__ block\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # 3 input channels, 6 output channels, 5x5 filter kernel, output => (32-5+2*0)/1 + 1 = 28\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 input channels (from first conv layer), 16 output channels, 5x5 filter kernel, output => (((28-2)/2 [This portion is from the first pooling] + 1)-5+2*0)/1 + 1 = 10\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2) # 2x2 kernel, 2 stride\n",
    "        self.fc1 = nn.Linear(16*5*5, 120) # 16 channel input where each channel is (10-2)/2 + 1 = 5x5 matrix; all values flattened out\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10) # Input size of L linear layer = Output size of L-1 linear layer; 10 classes require 10 outputs in the final linear layer  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv + relu + pool block 1 \n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.pool(out)\n",
    "        # Conv + relu + pool block 2\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.pool(out)\n",
    "        # Flatten the output so far\n",
    "        out = out.view(-1, 16*5*5) # Infer the number of rows automatically, but have 16*5*5 columns since the last output is of size [16, 5, 5]\n",
    "        # FC + relu block 1\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        # FC + relu block 2\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        # FC block 3\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fdf339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb603de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # Since this is a multiclass classification problem, CrossEntropyLoss is used; the output does not have to go through a softmax layer and the labels do not have to be one-hot encoded\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82082daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4, step: 200/12500, loss: 2.299\n",
      "Epoch: 1/4, step: 400/12500, loss: 2.254\n",
      "Epoch: 1/4, step: 600/12500, loss: 2.285\n",
      "Epoch: 1/4, step: 800/12500, loss: 2.354\n",
      "Epoch: 1/4, step: 1000/12500, loss: 2.320\n",
      "Epoch: 1/4, step: 1200/12500, loss: 2.279\n",
      "Epoch: 1/4, step: 1400/12500, loss: 2.334\n",
      "Epoch: 1/4, step: 1600/12500, loss: 2.270\n",
      "Epoch: 1/4, step: 1800/12500, loss: 2.324\n",
      "Epoch: 1/4, step: 2000/12500, loss: 2.321\n",
      "Epoch: 1/4, step: 2200/12500, loss: 2.305\n",
      "Epoch: 1/4, step: 2400/12500, loss: 2.328\n",
      "Epoch: 1/4, step: 2600/12500, loss: 2.351\n",
      "Epoch: 1/4, step: 2800/12500, loss: 2.289\n",
      "Epoch: 1/4, step: 3000/12500, loss: 2.285\n",
      "Epoch: 1/4, step: 3200/12500, loss: 2.274\n",
      "Epoch: 1/4, step: 3400/12500, loss: 2.297\n",
      "Epoch: 1/4, step: 3600/12500, loss: 2.329\n",
      "Epoch: 1/4, step: 3800/12500, loss: 2.283\n",
      "Epoch: 1/4, step: 4000/12500, loss: 2.284\n",
      "Epoch: 1/4, step: 4200/12500, loss: 2.310\n",
      "Epoch: 1/4, step: 4400/12500, loss: 2.344\n",
      "Epoch: 1/4, step: 4600/12500, loss: 2.305\n",
      "Epoch: 1/4, step: 4800/12500, loss: 2.301\n",
      "Epoch: 1/4, step: 5000/12500, loss: 2.299\n",
      "Epoch: 1/4, step: 5200/12500, loss: 2.255\n",
      "Epoch: 1/4, step: 5400/12500, loss: 2.250\n",
      "Epoch: 1/4, step: 5600/12500, loss: 2.297\n",
      "Epoch: 1/4, step: 5800/12500, loss: 2.308\n",
      "Epoch: 1/4, step: 6000/12500, loss: 2.317\n",
      "Epoch: 1/4, step: 6200/12500, loss: 2.285\n",
      "Epoch: 1/4, step: 6400/12500, loss: 2.301\n",
      "Epoch: 1/4, step: 6600/12500, loss: 2.278\n",
      "Epoch: 1/4, step: 6800/12500, loss: 2.304\n",
      "Epoch: 1/4, step: 7000/12500, loss: 2.275\n",
      "Epoch: 1/4, step: 7200/12500, loss: 2.274\n",
      "Epoch: 1/4, step: 7400/12500, loss: 2.297\n",
      "Epoch: 1/4, step: 7600/12500, loss: 2.319\n",
      "Epoch: 1/4, step: 7800/12500, loss: 2.257\n",
      "Epoch: 1/4, step: 8000/12500, loss: 2.293\n",
      "Epoch: 1/4, step: 8200/12500, loss: 2.214\n",
      "Epoch: 1/4, step: 8400/12500, loss: 2.275\n",
      "Epoch: 1/4, step: 8600/12500, loss: 2.204\n",
      "Epoch: 1/4, step: 8800/12500, loss: 2.326\n",
      "Epoch: 1/4, step: 9000/12500, loss: 2.259\n",
      "Epoch: 1/4, step: 9200/12500, loss: 2.332\n",
      "Epoch: 1/4, step: 9400/12500, loss: 2.149\n",
      "Epoch: 1/4, step: 9600/12500, loss: 2.029\n",
      "Epoch: 1/4, step: 9800/12500, loss: 2.231\n",
      "Epoch: 1/4, step: 10000/12500, loss: 2.191\n",
      "Epoch: 1/4, step: 10200/12500, loss: 2.261\n",
      "Epoch: 1/4, step: 10400/12500, loss: 2.201\n",
      "Epoch: 1/4, step: 10600/12500, loss: 2.117\n",
      "Epoch: 1/4, step: 10800/12500, loss: 2.254\n",
      "Epoch: 1/4, step: 11000/12500, loss: 2.002\n",
      "Epoch: 1/4, step: 11200/12500, loss: 2.067\n",
      "Epoch: 1/4, step: 11400/12500, loss: 2.351\n",
      "Epoch: 1/4, step: 11600/12500, loss: 1.969\n",
      "Epoch: 1/4, step: 11800/12500, loss: 1.897\n",
      "Epoch: 1/4, step: 12000/12500, loss: 1.929\n",
      "Epoch: 1/4, step: 12200/12500, loss: 2.251\n",
      "Epoch: 1/4, step: 12400/12500, loss: 1.765\n",
      "Epoch: 2/4, step: 200/12500, loss: 1.635\n",
      "Epoch: 2/4, step: 400/12500, loss: 1.946\n",
      "Epoch: 2/4, step: 600/12500, loss: 2.593\n",
      "Epoch: 2/4, step: 800/12500, loss: 1.824\n",
      "Epoch: 2/4, step: 1000/12500, loss: 2.484\n",
      "Epoch: 2/4, step: 1200/12500, loss: 1.894\n",
      "Epoch: 2/4, step: 1400/12500, loss: 2.024\n",
      "Epoch: 2/4, step: 1600/12500, loss: 1.576\n",
      "Epoch: 2/4, step: 1800/12500, loss: 1.822\n",
      "Epoch: 2/4, step: 2000/12500, loss: 1.874\n",
      "Epoch: 2/4, step: 2200/12500, loss: 1.927\n",
      "Epoch: 2/4, step: 2400/12500, loss: 2.777\n",
      "Epoch: 2/4, step: 2600/12500, loss: 2.044\n",
      "Epoch: 2/4, step: 2800/12500, loss: 2.247\n",
      "Epoch: 2/4, step: 3000/12500, loss: 2.005\n",
      "Epoch: 2/4, step: 3200/12500, loss: 2.084\n",
      "Epoch: 2/4, step: 3400/12500, loss: 2.379\n",
      "Epoch: 2/4, step: 3600/12500, loss: 2.329\n",
      "Epoch: 2/4, step: 3800/12500, loss: 1.424\n",
      "Epoch: 2/4, step: 4000/12500, loss: 1.627\n",
      "Epoch: 2/4, step: 4200/12500, loss: 1.890\n",
      "Epoch: 2/4, step: 4400/12500, loss: 2.006\n",
      "Epoch: 2/4, step: 4600/12500, loss: 2.269\n",
      "Epoch: 2/4, step: 4800/12500, loss: 2.161\n",
      "Epoch: 2/4, step: 5000/12500, loss: 1.498\n",
      "Epoch: 2/4, step: 5200/12500, loss: 1.776\n",
      "Epoch: 2/4, step: 5400/12500, loss: 2.317\n",
      "Epoch: 2/4, step: 5600/12500, loss: 1.588\n",
      "Epoch: 2/4, step: 5800/12500, loss: 1.770\n",
      "Epoch: 2/4, step: 6000/12500, loss: 1.552\n",
      "Epoch: 2/4, step: 6200/12500, loss: 2.219\n",
      "Epoch: 2/4, step: 6400/12500, loss: 1.675\n",
      "Epoch: 2/4, step: 6600/12500, loss: 1.725\n",
      "Epoch: 2/4, step: 6800/12500, loss: 2.765\n",
      "Epoch: 2/4, step: 7000/12500, loss: 1.107\n",
      "Epoch: 2/4, step: 7200/12500, loss: 1.993\n",
      "Epoch: 2/4, step: 7400/12500, loss: 1.946\n",
      "Epoch: 2/4, step: 7600/12500, loss: 1.933\n",
      "Epoch: 2/4, step: 7800/12500, loss: 1.871\n",
      "Epoch: 2/4, step: 8000/12500, loss: 1.677\n",
      "Epoch: 2/4, step: 8200/12500, loss: 1.810\n",
      "Epoch: 2/4, step: 8400/12500, loss: 1.541\n",
      "Epoch: 2/4, step: 8600/12500, loss: 2.771\n",
      "Epoch: 2/4, step: 8800/12500, loss: 1.270\n",
      "Epoch: 2/4, step: 9000/12500, loss: 1.382\n",
      "Epoch: 2/4, step: 9200/12500, loss: 2.004\n",
      "Epoch: 2/4, step: 9400/12500, loss: 1.335\n",
      "Epoch: 2/4, step: 9600/12500, loss: 1.553\n",
      "Epoch: 2/4, step: 9800/12500, loss: 1.874\n",
      "Epoch: 2/4, step: 10000/12500, loss: 1.545\n",
      "Epoch: 2/4, step: 10200/12500, loss: 1.603\n",
      "Epoch: 2/4, step: 10400/12500, loss: 1.698\n",
      "Epoch: 2/4, step: 10600/12500, loss: 1.804\n",
      "Epoch: 2/4, step: 10800/12500, loss: 1.505\n",
      "Epoch: 2/4, step: 11000/12500, loss: 1.631\n",
      "Epoch: 2/4, step: 11200/12500, loss: 2.166\n",
      "Epoch: 2/4, step: 11400/12500, loss: 2.493\n",
      "Epoch: 2/4, step: 11600/12500, loss: 1.506\n",
      "Epoch: 2/4, step: 11800/12500, loss: 1.438\n",
      "Epoch: 2/4, step: 12000/12500, loss: 1.548\n",
      "Epoch: 2/4, step: 12200/12500, loss: 1.666\n",
      "Epoch: 2/4, step: 12400/12500, loss: 1.492\n",
      "Epoch: 3/4, step: 200/12500, loss: 1.860\n",
      "Epoch: 3/4, step: 400/12500, loss: 1.502\n",
      "Epoch: 3/4, step: 600/12500, loss: 1.556\n",
      "Epoch: 3/4, step: 800/12500, loss: 1.995\n",
      "Epoch: 3/4, step: 1000/12500, loss: 1.626\n",
      "Epoch: 3/4, step: 1200/12500, loss: 2.086\n",
      "Epoch: 3/4, step: 1400/12500, loss: 2.345\n",
      "Epoch: 3/4, step: 1600/12500, loss: 1.419\n",
      "Epoch: 3/4, step: 1800/12500, loss: 1.858\n",
      "Epoch: 3/4, step: 2000/12500, loss: 1.393\n",
      "Epoch: 3/4, step: 2200/12500, loss: 1.648\n",
      "Epoch: 3/4, step: 2400/12500, loss: 1.614\n",
      "Epoch: 3/4, step: 2600/12500, loss: 1.137\n",
      "Epoch: 3/4, step: 2800/12500, loss: 2.736\n",
      "Epoch: 3/4, step: 3000/12500, loss: 1.752\n",
      "Epoch: 3/4, step: 3200/12500, loss: 1.683\n",
      "Epoch: 3/4, step: 3400/12500, loss: 1.870\n",
      "Epoch: 3/4, step: 3600/12500, loss: 1.711\n",
      "Epoch: 3/4, step: 3800/12500, loss: 0.943\n",
      "Epoch: 3/4, step: 4000/12500, loss: 2.465\n",
      "Epoch: 3/4, step: 4200/12500, loss: 1.172\n",
      "Epoch: 3/4, step: 4400/12500, loss: 1.891\n",
      "Epoch: 3/4, step: 4600/12500, loss: 1.569\n",
      "Epoch: 3/4, step: 4800/12500, loss: 1.855\n",
      "Epoch: 3/4, step: 5000/12500, loss: 2.065\n",
      "Epoch: 3/4, step: 5200/12500, loss: 1.598\n",
      "Epoch: 3/4, step: 5400/12500, loss: 1.516\n",
      "Epoch: 3/4, step: 5600/12500, loss: 1.760\n",
      "Epoch: 3/4, step: 5800/12500, loss: 1.158\n",
      "Epoch: 3/4, step: 6000/12500, loss: 1.334\n",
      "Epoch: 3/4, step: 6200/12500, loss: 1.563\n",
      "Epoch: 3/4, step: 6400/12500, loss: 2.676\n",
      "Epoch: 3/4, step: 6600/12500, loss: 1.422\n",
      "Epoch: 3/4, step: 6800/12500, loss: 1.436\n",
      "Epoch: 3/4, step: 7000/12500, loss: 1.787\n",
      "Epoch: 3/4, step: 7200/12500, loss: 1.242\n",
      "Epoch: 3/4, step: 7400/12500, loss: 1.477\n",
      "Epoch: 3/4, step: 7600/12500, loss: 0.989\n",
      "Epoch: 3/4, step: 7800/12500, loss: 1.835\n",
      "Epoch: 3/4, step: 8000/12500, loss: 3.063\n",
      "Epoch: 3/4, step: 8200/12500, loss: 1.342\n",
      "Epoch: 3/4, step: 8400/12500, loss: 1.638\n",
      "Epoch: 3/4, step: 8600/12500, loss: 1.090\n",
      "Epoch: 3/4, step: 8800/12500, loss: 1.187\n",
      "Epoch: 3/4, step: 9000/12500, loss: 1.055\n",
      "Epoch: 3/4, step: 9200/12500, loss: 1.308\n",
      "Epoch: 3/4, step: 9400/12500, loss: 2.288\n",
      "Epoch: 3/4, step: 9600/12500, loss: 2.229\n",
      "Epoch: 3/4, step: 9800/12500, loss: 1.447\n",
      "Epoch: 3/4, step: 10000/12500, loss: 1.382\n",
      "Epoch: 3/4, step: 10200/12500, loss: 1.905\n",
      "Epoch: 3/4, step: 10400/12500, loss: 1.462\n",
      "Epoch: 3/4, step: 10600/12500, loss: 1.458\n",
      "Epoch: 3/4, step: 10800/12500, loss: 1.675\n",
      "Epoch: 3/4, step: 11000/12500, loss: 1.363\n",
      "Epoch: 3/4, step: 11200/12500, loss: 1.633\n",
      "Epoch: 3/4, step: 11400/12500, loss: 1.240\n",
      "Epoch: 3/4, step: 11600/12500, loss: 1.423\n",
      "Epoch: 3/4, step: 11800/12500, loss: 1.775\n",
      "Epoch: 3/4, step: 12000/12500, loss: 1.265\n",
      "Epoch: 3/4, step: 12200/12500, loss: 2.303\n",
      "Epoch: 3/4, step: 12400/12500, loss: 1.278\n",
      "Epoch: 4/4, step: 200/12500, loss: 0.372\n",
      "Epoch: 4/4, step: 400/12500, loss: 1.266\n",
      "Epoch: 4/4, step: 600/12500, loss: 1.504\n",
      "Epoch: 4/4, step: 800/12500, loss: 1.539\n",
      "Epoch: 4/4, step: 1000/12500, loss: 1.506\n",
      "Epoch: 4/4, step: 1200/12500, loss: 1.324\n",
      "Epoch: 4/4, step: 1400/12500, loss: 1.421\n",
      "Epoch: 4/4, step: 1600/12500, loss: 0.907\n",
      "Epoch: 4/4, step: 1800/12500, loss: 1.696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/4, step: 2000/12500, loss: 2.348\n",
      "Epoch: 4/4, step: 2200/12500, loss: 1.323\n",
      "Epoch: 4/4, step: 2400/12500, loss: 1.445\n",
      "Epoch: 4/4, step: 2600/12500, loss: 1.573\n",
      "Epoch: 4/4, step: 2800/12500, loss: 2.082\n",
      "Epoch: 4/4, step: 3000/12500, loss: 0.785\n",
      "Epoch: 4/4, step: 3200/12500, loss: 1.319\n",
      "Epoch: 4/4, step: 3400/12500, loss: 1.214\n",
      "Epoch: 4/4, step: 3600/12500, loss: 1.817\n",
      "Epoch: 4/4, step: 3800/12500, loss: 1.578\n",
      "Epoch: 4/4, step: 4000/12500, loss: 1.904\n",
      "Epoch: 4/4, step: 4200/12500, loss: 1.794\n",
      "Epoch: 4/4, step: 4400/12500, loss: 1.651\n",
      "Epoch: 4/4, step: 4600/12500, loss: 1.111\n",
      "Epoch: 4/4, step: 4800/12500, loss: 1.136\n",
      "Epoch: 4/4, step: 5000/12500, loss: 1.122\n",
      "Epoch: 4/4, step: 5200/12500, loss: 0.961\n",
      "Epoch: 4/4, step: 5400/12500, loss: 1.262\n",
      "Epoch: 4/4, step: 5600/12500, loss: 1.330\n",
      "Epoch: 4/4, step: 5800/12500, loss: 0.641\n",
      "Epoch: 4/4, step: 6000/12500, loss: 0.662\n",
      "Epoch: 4/4, step: 6200/12500, loss: 2.218\n",
      "Epoch: 4/4, step: 6400/12500, loss: 0.792\n",
      "Epoch: 4/4, step: 6600/12500, loss: 1.051\n",
      "Epoch: 4/4, step: 6800/12500, loss: 0.628\n",
      "Epoch: 4/4, step: 7000/12500, loss: 1.614\n",
      "Epoch: 4/4, step: 7200/12500, loss: 1.854\n",
      "Epoch: 4/4, step: 7400/12500, loss: 1.183\n",
      "Epoch: 4/4, step: 7600/12500, loss: 2.050\n",
      "Epoch: 4/4, step: 7800/12500, loss: 0.980\n",
      "Epoch: 4/4, step: 8000/12500, loss: 0.956\n",
      "Epoch: 4/4, step: 8200/12500, loss: 1.481\n",
      "Epoch: 4/4, step: 8400/12500, loss: 1.282\n",
      "Epoch: 4/4, step: 8600/12500, loss: 1.491\n",
      "Epoch: 4/4, step: 8800/12500, loss: 2.588\n",
      "Epoch: 4/4, step: 9000/12500, loss: 1.151\n",
      "Epoch: 4/4, step: 9200/12500, loss: 1.212\n",
      "Epoch: 4/4, step: 9400/12500, loss: 1.490\n",
      "Epoch: 4/4, step: 9600/12500, loss: 0.726\n",
      "Epoch: 4/4, step: 9800/12500, loss: 1.809\n",
      "Epoch: 4/4, step: 10000/12500, loss: 1.541\n",
      "Epoch: 4/4, step: 10200/12500, loss: 0.850\n",
      "Epoch: 4/4, step: 10400/12500, loss: 2.489\n",
      "Epoch: 4/4, step: 10600/12500, loss: 1.689\n",
      "Epoch: 4/4, step: 10800/12500, loss: 1.783\n",
      "Epoch: 4/4, step: 11000/12500, loss: 1.099\n",
      "Epoch: 4/4, step: 11200/12500, loss: 1.439\n",
      "Epoch: 4/4, step: 11400/12500, loss: 1.943\n",
      "Epoch: 4/4, step: 11600/12500, loss: 1.691\n",
      "Epoch: 4/4, step: 11800/12500, loss: 1.385\n",
      "Epoch: 4/4, step: 12000/12500, loss: 1.591\n",
      "Epoch: 4/4, step: 12200/12500, loss: 2.184\n",
      "Epoch: 4/4, step: 12400/12500, loss: 1.424\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_iters = len(train_dataloader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        # Transfer inputs and labels to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predicted_outputs = model(images)\n",
    "        \n",
    "        # Loss calculation\n",
    "        loss = criterion(predicted_outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Parameter update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print(f\"Epoch: {epoch+1}/{num_epochs}, step: {i+1}/{num_iters}, loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47078907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 40.379%\n",
      "Classwise accuracy\n",
      "Accuracy for plane: 43.000%\n",
      "Accuracy for car: 52.600%\n",
      "Accuracy for bird: 43.600%\n",
      "Accuracy for cat: 27.200%\n",
      "Accuracy for deer: 47.600%\n",
      "Accuracy for dog: 41.700%\n",
      "Accuracy for frog: 40.900%\n",
      "Accuracy for horse: 53.800%\n",
      "Accuracy for ship: 51.100%\n",
      "Accuracy for truck: 54.500%\n"
     ]
    }
   ],
   "source": [
    "# Testing and evaluation\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)] # Array to store number of correctly classified samples for each of the 10 classes\n",
    "    n_class_samples = [0 for i in range(10)] # Array to store number of samples for each of the 10 classes\n",
    "    \n",
    "    for i, (images, labels) in enumerate(test_dataloader):\n",
    "        # Transfer inputs and labels to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Predictions\n",
    "        predicted_outputs = model(images)\n",
    "        \n",
    "        # Overall accuracy stats\n",
    "        _, predicted_output = torch.max(predicted_outputs, 1) # Get the index of the highest value (this is the predicted class)\n",
    "        n_samples += labels[0] # Increase the number of samples predicted by the number of samples in this batch\n",
    "        n_correct += (predicted_output==labels).sum().item() # Get the number of correctly classified samples in this batch\n",
    "        \n",
    "        # Classwise accuracy stats\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i] # Get the true label of the sample\n",
    "            pred = predicted_output[i] # Get the predicted label of the sample\n",
    "            \n",
    "            if (label==pred):\n",
    "                n_class_correct[label] += 1 # If correctly classified, increase count by 1\n",
    "            \n",
    "            n_class_samples[label] += 1 # Increase the count of samples for the true label class\n",
    "            \n",
    "    overall_accuracy = n_correct / n_samples * 100.0\n",
    "    print(f\"Overall accuracy: {overall_accuracy:.3f}%\")\n",
    "    print(f\"Classwise accuracy\")\n",
    "    for i in range(10):\n",
    "        class_accuracy = n_class_correct[i] / n_class_samples[i] * 100.0\n",
    "        print(f\"Accuracy for {classes[i]}: {class_accuracy:.3f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

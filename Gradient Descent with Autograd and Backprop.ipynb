{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3ab100",
   "metadata": {},
   "source": [
    "### Gradient Descent with Autograd and Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa3af6",
   "metadata": {},
   "source": [
    "Here's a model built completely from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7133b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume a simple linear regression model\n",
    "# The equation for that can be modeled as\n",
    "# y = Wx + b = 2x + 3\n",
    "\n",
    "# Inputs\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "\n",
    "# Outputs\n",
    "Y = np.array([5, 7, 9, 11], dtype=np.float32)\n",
    "\n",
    "# Weights and biases\n",
    "W = 0 # Assume that we start with 0 weight initially and there is only one input dimension\n",
    "b = 0 # Assume that we start with 0 bias initially\n",
    "\n",
    "# Predict\n",
    "def forward(W, X, b):\n",
    "    return W*X+b\n",
    "\n",
    "# Loss\n",
    "# Assume we are going with L = MSE\n",
    "def loss(Y, Yhat):\n",
    "    return ((Yhat - Y)**2).mean()\n",
    "\n",
    "# Gradient - dL\n",
    "def gradient(X, Y, Yhat):\n",
    "    dW = np.dot(-2*X, Y-Yhat).mean()\n",
    "    db = -2*((Y-Yhat)).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54384754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual value: [ 5.  7.  9. 11.]\n",
      "Prediction before training: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Pretraining prediction\n",
    "print(\"Actual value: \" + str(Y))\n",
    "print(\"Prediction before training: \" + str(forward(W, X, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c79a2ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Prediction: [0. 0. 0. 0.]\n",
      "Loss: 69.0\n",
      "Epoch 2\n",
      "Prediction: [1.9599999 3.76      5.5599995 7.3599997]\n",
      "Loss: 11.205602\n",
      "Epoch 3\n",
      "Prediction: [ 2.7148     5.2028     7.6907997 10.1788   ]\n",
      "Loss: 2.710111\n",
      "Epoch 4\n",
      "Prediction: [ 3.007704  5.757544  8.507384 11.257224]\n",
      "Loss: 1.4554435\n",
      "Epoch 5\n",
      "Prediction: [ 3.1235778  5.971941   8.820305  11.668668 ]\n",
      "Loss: 1.2643182\n",
      "Epoch 6\n",
      "Prediction: [ 3.1715946  6.055897   8.9402    11.824502 ]\n",
      "Loss: 1.2294439\n",
      "Epoch 7\n",
      "Prediction: [ 3.1935937  6.089856   8.986118  11.8823805]\n",
      "Loss: 1.2175634\n",
      "Epoch 8\n",
      "Prediction: [ 3.2056103  6.104649   9.003689  11.902727 ]\n",
      "Loss: 1.2091044\n",
      "Epoch 9\n",
      "Prediction: [ 3.2137892  6.1120906  9.010391  11.908692 ]\n",
      "Loss: 1.2011905\n",
      "Epoch 10\n",
      "Prediction: [ 3.2204862  6.116709   9.012932  11.909155 ]\n",
      "Loss: 1.1934005\n",
      "Epoch 11\n",
      "Prediction: [ 3.2266035  6.1202397  9.013876  11.907513 ]\n",
      "Loss: 1.1856713\n",
      "Epoch 12\n",
      "Prediction: [ 3.232487  6.123348  9.01421  11.905071]\n",
      "Loss: 1.1779941\n",
      "Epoch 13\n",
      "Prediction: [ 3.2382696  6.126289   9.014308  11.902328 ]\n",
      "Loss: 1.1703663\n",
      "Epoch 14\n",
      "Prediction: [ 3.2440019  6.1291595  9.014317  11.899474 ]\n",
      "Loss: 1.1627879\n",
      "Epoch 15\n",
      "Prediction: [ 3.2497036  6.131998   9.014293  11.896586 ]\n",
      "Loss: 1.155259\n",
      "Epoch 16\n",
      "Prediction: [ 3.2553823  6.134818   9.014255  11.89369  ]\n",
      "Loss: 1.147779\n",
      "Epoch 17\n",
      "Prediction: [ 3.261041   6.1376257  9.014211  11.890796 ]\n",
      "Loss: 1.1403468\n",
      "Epoch 18\n",
      "Prediction: [ 3.2666802  6.1404233  9.014166  11.887909 ]\n",
      "Loss: 1.1329632\n",
      "Epoch 19\n",
      "Prediction: [ 3.2723012  6.143211   9.014121  11.885031 ]\n",
      "Loss: 1.1256274\n",
      "Epoch 20\n",
      "Prediction: [ 3.2779036  6.145989   9.014074  11.88216  ]\n",
      "Loss: 1.118339\n",
      "Epoch 21\n",
      "Prediction: [ 3.2834883  6.148759   9.0140295 11.8793   ]\n",
      "Loss: 1.1110973\n",
      "Epoch 22\n",
      "Prediction: [ 3.2890544  6.1515193  9.013984  11.876449 ]\n",
      "Loss: 1.1039029\n",
      "Epoch 23\n",
      "Prediction: [ 3.2946024  6.15427    9.013938  11.873606 ]\n",
      "Loss: 1.0967554\n",
      "Epoch 24\n",
      "Prediction: [ 3.3001328  6.157013   9.013893  11.870773 ]\n",
      "Loss: 1.0896537\n",
      "Epoch 25\n",
      "Prediction: [ 3.3056452  6.159747   9.013849  11.86795  ]\n",
      "Loss: 1.0825982\n",
      "Epoch 26\n",
      "Prediction: [ 3.3111396  6.162472   9.0138035 11.865135 ]\n",
      "Loss: 1.0755881\n",
      "Epoch 27\n",
      "Prediction: [ 3.316616   6.165187   9.013759  11.8623295]\n",
      "Loss: 1.068624\n",
      "Epoch 28\n",
      "Prediction: [ 3.322075   6.1678944  9.013714  11.859533 ]\n",
      "Loss: 1.0617045\n",
      "Epoch 29\n",
      "Prediction: [ 3.327516  6.170593  9.013669 11.856746]\n",
      "Loss: 1.0548297\n",
      "Epoch 30\n",
      "Prediction: [ 3.3329396  6.1732826  9.013625  11.853969 ]\n",
      "Loss: 1.0480001\n",
      "Epoch 31\n",
      "Prediction: [ 3.3383455  6.1759634  9.013581  11.851199 ]\n",
      "Loss: 1.0412141\n",
      "Epoch 32\n",
      "Prediction: [ 3.3437338  6.1786356  9.013536  11.848438 ]\n",
      "Loss: 1.034472\n",
      "Epoch 33\n",
      "Prediction: [ 3.3491046  6.1812987  9.013493  11.845687 ]\n",
      "Loss: 1.0277739\n",
      "Epoch 34\n",
      "Prediction: [ 3.3544583  6.183954   9.01345   11.842945 ]\n",
      "Loss: 1.021119\n",
      "Epoch 35\n",
      "Prediction: [ 3.3597944  6.1866     9.013406  11.840212 ]\n",
      "Loss: 1.0145073\n",
      "Epoch 36\n",
      "Prediction: [ 3.365113   6.1892376  9.013363  11.837486 ]\n",
      "Loss: 1.0079383\n",
      "Epoch 37\n",
      "Prediction: [ 3.3704147  6.191867   9.013319  11.834771 ]\n",
      "Loss: 1.0014119\n",
      "Epoch 38\n",
      "Prediction: [ 3.3756988  6.194487   9.013275  11.832064 ]\n",
      "Loss: 0.9949278\n",
      "Epoch 39\n",
      "Prediction: [ 3.3809662  6.1970997  9.013232  11.829366 ]\n",
      "Loss: 0.98848546\n",
      "Epoch 40\n",
      "Prediction: [ 3.3862164  6.199703   9.01319   11.826677 ]\n",
      "Loss: 0.98208547\n",
      "Epoch 41\n",
      "Prediction: [ 3.3914495  6.202298   9.013147  11.823996 ]\n",
      "Loss: 0.9757261\n",
      "Epoch 42\n",
      "Prediction: [ 3.3966656  6.2048845  9.013104  11.821323 ]\n",
      "Loss: 0.9694084\n",
      "Epoch 43\n",
      "Prediction: [ 3.401865   6.2074633  9.0130625 11.818661 ]\n",
      "Loss: 0.9631315\n",
      "Epoch 44\n",
      "Prediction: [ 3.407047  6.210033  9.013019 11.816005]\n",
      "Loss: 0.9568951\n",
      "Epoch 45\n",
      "Prediction: [ 3.4122128  6.212595   9.012977  11.813359 ]\n",
      "Loss: 0.9506991\n",
      "Epoch 46\n",
      "Prediction: [ 3.4173617  6.215149   9.012936  11.810722 ]\n",
      "Loss: 0.9445433\n",
      "Epoch 47\n",
      "Prediction: [ 3.4224937  6.2176933  9.012894  11.808092 ]\n",
      "Loss: 0.9384272\n",
      "Epoch 48\n",
      "Prediction: [ 3.4276094  6.2202306  9.012851  11.805472 ]\n",
      "Loss: 0.9323509\n",
      "Epoch 49\n",
      "Prediction: [ 3.4327083  6.2227592  9.012811  11.802861 ]\n",
      "Loss: 0.92631423\n",
      "Epoch 50\n",
      "Prediction: [ 3.4377904  6.225279   9.012768  11.800257 ]\n",
      "Loss: 0.9203164\n",
      "Epoch 51\n",
      "Prediction: [ 3.4428563  6.227792   9.012727  11.797662 ]\n",
      "Loss: 0.91435707\n",
      "Epoch 52\n",
      "Prediction: [ 3.447906   6.2302957  9.012686  11.795075 ]\n",
      "Loss: 0.9084366\n",
      "Epoch 53\n",
      "Prediction: [ 3.452939  6.232792  9.012645 11.792498]\n",
      "Loss: 0.9025546\n",
      "Epoch 54\n",
      "Prediction: [ 3.4579556  6.235279   9.012603  11.789927 ]\n",
      "Loss: 0.8967104\n",
      "Epoch 55\n",
      "Prediction: [ 3.4629562  6.2377596  9.012563  11.787366 ]\n",
      "Loss: 0.8909043\n",
      "Epoch 56\n",
      "Prediction: [ 3.4679406  6.240231   9.012522  11.784813 ]\n",
      "Loss: 0.88513577\n",
      "Epoch 57\n",
      "Prediction: [ 3.4729085  6.242695   9.012481  11.782268 ]\n",
      "Loss: 0.8794044\n",
      "Epoch 58\n",
      "Prediction: [ 3.4778607  6.2451506  9.012441  11.779731 ]\n",
      "Loss: 0.87371016\n",
      "Epoch 59\n",
      "Prediction: [ 3.4827967  6.2475986  9.012401  11.777203 ]\n",
      "Loss: 0.86805284\n",
      "Epoch 60\n",
      "Prediction: [ 3.4877167  6.2500386  9.012361  11.774682 ]\n",
      "Loss: 0.862432\n",
      "Epoch 61\n",
      "Prediction: [ 3.4926205  6.25247    9.0123205 11.77217  ]\n",
      "Loss: 0.8568481\n",
      "Epoch 62\n",
      "Prediction: [ 3.4975085  6.2548943  9.01228   11.769666 ]\n",
      "Loss: 0.8512998\n",
      "Epoch 63\n",
      "Prediction: [ 3.5023808  6.2573104  9.01224   11.76717  ]\n",
      "Loss: 0.84578764\n",
      "Epoch 64\n",
      "Prediction: [ 3.5072372  6.259719   9.0122    11.764682 ]\n",
      "Loss: 0.840311\n",
      "Epoch 65\n",
      "Prediction: [ 3.5120778  6.2621193  9.01216   11.762202 ]\n",
      "Loss: 0.8348701\n",
      "Epoch 66\n",
      "Prediction: [ 3.516903  6.264512  9.012122 11.759731]\n",
      "Loss: 0.82946444\n",
      "Epoch 67\n",
      "Prediction: [ 3.5217123  6.266897   9.012083  11.757268 ]\n",
      "Loss: 0.8240937\n",
      "Epoch 68\n",
      "Prediction: [ 3.526506  6.269274  9.012043 11.754811]\n",
      "Loss: 0.8187575\n",
      "Epoch 69\n",
      "Prediction: [ 3.531284  6.271644  9.012004 11.752364]\n",
      "Loss: 0.8134562\n",
      "Epoch 70\n",
      "Prediction: [ 3.5360467  6.274006   9.011966  11.749925 ]\n",
      "Loss: 0.8081892\n",
      "Epoch 71\n",
      "Prediction: [ 3.5407941  6.27636    9.011927  11.747493 ]\n",
      "Loss: 0.80295604\n",
      "Epoch 72\n",
      "Prediction: [ 3.5455258  6.2787066  9.011888  11.745069 ]\n",
      "Loss: 0.79775697\n",
      "Epoch 73\n",
      "Prediction: [ 3.5502424  6.2810454  9.011848  11.742652 ]\n",
      "Loss: 0.79259133\n",
      "Epoch 74\n",
      "Prediction: [ 3.5549436  6.2833767  9.01181   11.740244 ]\n",
      "Loss: 0.7874594\n",
      "Epoch 75\n",
      "Prediction: [ 3.5596297  6.285701   9.011772  11.7378435]\n",
      "Loss: 0.78236043\n",
      "Epoch 76\n",
      "Prediction: [ 3.5643005  6.2880177  9.011735  11.735452 ]\n",
      "Loss: 0.7772947\n",
      "Epoch 77\n",
      "Prediction: [ 3.5689561  6.290326   9.011696  11.733066 ]\n",
      "Loss: 0.77226144\n",
      "Epoch 78\n",
      "Prediction: [ 3.5735967  6.2926273  9.011659  11.730689 ]\n",
      "Loss: 0.76726115\n",
      "Epoch 79\n",
      "Prediction: [ 3.578222   6.2949214  9.01162   11.728319 ]\n",
      "Loss: 0.7622931\n",
      "Epoch 80\n",
      "Prediction: [ 3.5828328  6.297208   9.011583  11.725959 ]\n",
      "Loss: 0.7573575\n",
      "Epoch 81\n",
      "Prediction: [ 3.587428   6.2994866  9.011545  11.723604 ]\n",
      "Loss: 0.7524536\n",
      "Epoch 82\n",
      "Prediction: [ 3.5920088  6.3017583  9.011508  11.721257 ]\n",
      "Loss: 0.74758124\n",
      "Epoch 83\n",
      "Prediction: [ 3.5965743  6.3040223  9.01147   11.718918 ]\n",
      "Loss: 0.74274075\n",
      "Epoch 84\n",
      "Prediction: [ 3.6011257  6.3062797  9.011433  11.716587 ]\n",
      "Loss: 0.73793113\n",
      "Epoch 85\n",
      "Prediction: [ 3.6056619  6.3085294  9.011396  11.714264 ]\n",
      "Loss: 0.73315334\n",
      "Epoch 86\n",
      "Prediction: [ 3.6101832  6.3107715  9.011359  11.711947 ]\n",
      "Loss: 0.7284062\n",
      "Epoch 87\n",
      "Prediction: [ 3.6146903  6.3130064  9.011322  11.709639 ]\n",
      "Loss: 0.7236896\n",
      "Epoch 88\n",
      "Prediction: [ 3.6191823  6.315234   9.011286  11.707337 ]\n",
      "Loss: 0.71900374\n",
      "Epoch 89\n",
      "Prediction: [ 3.62366    6.3174543  9.01125   11.705044 ]\n",
      "Loss: 0.7143484\n",
      "Epoch 90\n",
      "Prediction: [ 3.628123  6.319668  9.011212 11.702757]\n",
      "Loss: 0.70972276\n",
      "Epoch 91\n",
      "Prediction: [ 3.632572  6.321874  9.011176 11.700479]\n",
      "Loss: 0.7051273\n",
      "Epoch 92\n",
      "Prediction: [ 3.637006  6.324073  9.011139 11.698206]\n",
      "Loss: 0.7005614\n",
      "Epoch 93\n",
      "Prediction: [ 3.641426   6.3262653  9.011104  11.695943 ]\n",
      "Loss: 0.69602525\n",
      "Epoch 94\n",
      "Prediction: [ 3.6458316  6.3284497  9.011068  11.693686 ]\n",
      "Loss: 0.69151855\n",
      "Epoch 95\n",
      "Prediction: [ 3.6502228  6.3306274  9.011032  11.691437 ]\n",
      "Loss: 0.68704116\n",
      "Epoch 96\n",
      "Prediction: [ 3.6546001  6.3327985  9.010997  11.689195 ]\n",
      "Loss: 0.68259215\n",
      "Epoch 97\n",
      "Prediction: [ 3.6589627  6.3349614  9.010961  11.686959 ]\n",
      "Loss: 0.67817265\n",
      "Epoch 98\n",
      "Prediction: [ 3.6633115  6.337118   9.010925  11.684732 ]\n",
      "Loss: 0.67378163\n",
      "Epoch 99\n",
      "Prediction: [ 3.667646   6.3392677  9.01089   11.682511 ]\n",
      "Loss: 0.6694187\n",
      "Epoch 100\n",
      "Prediction: [ 3.6719663  6.34141    9.010854  11.680298 ]\n",
      "Loss: 0.66508424\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 100 # Converges for sure at 100000 iterations at alpha = 0.01; could experiment a little more\n",
    "alpha = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch \" + str(epoch+1))\n",
    "    Yhat = forward(W, X, b)\n",
    "    print(\"Prediction: \" + str(Yhat))\n",
    "    prediction_loss = loss(Y, Yhat)\n",
    "    print(\"Loss: \" + str(prediction_loss))\n",
    "    dW, db = gradient(X, Y, Yhat)\n",
    "    W -= alpha * dW\n",
    "    b -= alpha * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127afb4",
   "metadata": {},
   "source": [
    "Obviously this is far too much work for us. We are going to incorporate a great degree of library functions to make our lives easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2a05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assume the same linear regression model\n",
    "# y = Wx + b = 2x + 3\n",
    "\n",
    "# Inputs\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "\n",
    "# Outputs\n",
    "Y = torch.tensor([5, 7, 9, 11], dtype=torch.float32)\n",
    "\n",
    "# Weights and biases\n",
    "# We need to calculate the gradient of the loss function with respect\n",
    "# to the weights and biases. So we need to add the requires_grad=True\n",
    "# flag to the tensors\n",
    "W = torch.tensor(0, dtype=torch.float32, requires_grad=True) # Assume that we start with 0 weight initially and there is only one input dimension\n",
    "b = torch.tensor(0, dtype=torch.float32, requires_grad=True) # Assume that we start with 0 bias initially\n",
    "\n",
    "# Predict\n",
    "def forward(W, X, b):\n",
    "    return W*X+b\n",
    "\n",
    "# Loss\n",
    "# Assume we are going with L = MSE\n",
    "def loss(Y, Yhat):\n",
    "    return ((Yhat - Y)**2).mean()\n",
    "\n",
    "# We do not need this\n",
    "# This will be handled by the autograd library in PyTorch\n",
    "# # Gradient - dL\n",
    "# def gradient(X, Y, Yhat):\n",
    "#     dW = np.dot(-2*X, Y-Yhat).mean()\n",
    "#     db = -2*((Y-Yhat)).mean()\n",
    "#     return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e95a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Prediction: tensor([0., 0., 0., 0.], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(69., grad_fn=<MeanBackward0>)\n",
      "Epoch 2\n",
      "Prediction: tensor([ 6.1000, 10.6000, 15.1000, 19.6000], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(31.3350, grad_fn=<MeanBackward0>)\n",
      "Epoch 3\n",
      "Prediction: tensor([2.0800, 3.5300, 4.9800, 6.4300], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(14.4032, grad_fn=<MeanBackward0>)\n",
      "Epoch 4\n",
      "Prediction: tensor([ 4.8390,  8.2990, 11.7590, 15.2190], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(6.7813, grad_fn=<MeanBackward0>)\n",
      "Epoch 5\n",
      "Prediction: tensor([3.0537, 5.1342, 7.2147, 9.2952], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(3.3407, grad_fn=<MeanBackward0>)\n",
      "Epoch 6\n",
      "Prediction: tensor([ 4.3115,  7.2846, 10.2578, 13.2309], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.7785, grad_fn=<MeanBackward0>)\n",
      "Epoch 7\n",
      "Prediction: tensor([ 3.5283,  5.8726,  8.2169, 10.5612], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.0607, grad_fn=<MeanBackward0>)\n",
      "Epoch 8\n",
      "Prediction: tensor([ 4.1110,  6.8468,  9.5826, 12.3184], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.7229, grad_fn=<MeanBackward0>)\n",
      "Epoch 9\n",
      "Prediction: tensor([ 3.7767,  6.2212,  8.6658, 11.1103], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.5567, grad_fn=<MeanBackward0>)\n",
      "Epoch 10\n",
      "Prediction: tensor([ 4.0551,  6.6668,  9.2784, 11.8901], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.4684, grad_fn=<MeanBackward0>)\n",
      "Epoch 11\n",
      "Prediction: tensor([ 3.9214,  6.3938,  8.8663, 11.3387], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.4159, grad_fn=<MeanBackward0>)\n",
      "Epoch 12\n",
      "Prediction: tensor([ 4.0623,  6.6016,  9.1409, 11.6802], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.3801, grad_fn=<MeanBackward0>)\n",
      "Epoch 13\n",
      "Prediction: tensor([ 4.0176,  6.4865,  8.9553, 11.4242], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.3527, grad_fn=<MeanBackward0>)\n",
      "Epoch 14\n",
      "Prediction: tensor([ 4.0957,  6.5869,  9.0782, 11.5694], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.3296, grad_fn=<MeanBackward0>)\n",
      "Epoch 15\n",
      "Prediction: tensor([ 4.0902,  6.5423,  8.9944, 11.4465], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.3092, grad_fn=<MeanBackward0>)\n",
      "Epoch 16\n",
      "Prediction: tensor([ 4.1393,  6.5942,  9.0491, 11.5040], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.2905, grad_fn=<MeanBackward0>)\n",
      "Epoch 17\n",
      "Prediction: tensor([ 4.1504,  6.5807,  9.0111, 11.4415], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.2732, grad_fn=<MeanBackward0>)\n",
      "Epoch 18\n",
      "Prediction: tensor([ 4.1856,  6.6105,  9.0353, 11.4601], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.2570, grad_fn=<MeanBackward0>)\n",
      "Epoch 19\n",
      "Prediction: tensor([ 4.2034,  6.6106,  9.0178, 11.4250], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.2418, grad_fn=<MeanBackward0>)\n",
      "Epoch 20\n",
      "Prediction: tensor([ 4.2317,  6.6300,  9.0283, 11.4265], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.2275, grad_fn=<MeanBackward0>)\n",
      "Epoch 21\n",
      "Prediction: tensor([ 4.2517,  6.6359,  9.0200, 11.4042], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.2141, grad_fn=<MeanBackward0>)\n",
      "Epoch 22\n",
      "Prediction: tensor([ 4.2761,  6.6503,  9.0244, 11.3985], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.2014, grad_fn=<MeanBackward0>)\n",
      "Epoch 23\n",
      "Prediction: tensor([ 4.2965,  6.6584,  9.0203, 11.3823], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.1895, grad_fn=<MeanBackward0>)\n",
      "Epoch 24\n",
      "Prediction: tensor([ 4.3184,  6.6702,  9.0220, 11.3737], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.1784, grad_fn=<MeanBackward0>)\n",
      "Epoch 25\n",
      "Prediction: tensor([ 4.3382,  6.6790,  9.0198, 11.3606], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.1678, grad_fn=<MeanBackward0>)\n",
      "Epoch 26\n",
      "Prediction: tensor([ 4.3584,  6.6893,  9.0202, 11.3511], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.1579, grad_fn=<MeanBackward0>)\n",
      "Epoch 27\n",
      "Prediction: tensor([ 4.3774,  6.6982,  9.0189, 11.3397], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.1486, grad_fn=<MeanBackward0>)\n",
      "Epoch 28\n",
      "Prediction: tensor([ 4.3962,  6.7075,  9.0188, 11.3301], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.1399, grad_fn=<MeanBackward0>)\n",
      "Epoch 29\n",
      "Prediction: tensor([ 4.4142,  6.7161,  9.0180, 11.3199], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.1316, grad_fn=<MeanBackward0>)\n",
      "Epoch 30\n",
      "Prediction: tensor([ 4.4318,  6.7247,  9.0176, 11.3105], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.1239, grad_fn=<MeanBackward0>)\n",
      "Epoch 31\n",
      "Prediction: tensor([ 4.4487,  6.7329,  9.0170, 11.3011], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.1165, grad_fn=<MeanBackward0>)\n",
      "Epoch 32\n",
      "Prediction: tensor([ 4.4653,  6.7409,  9.0165, 11.2922], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.1097, grad_fn=<MeanBackward0>)\n",
      "Epoch 33\n",
      "Prediction: tensor([ 4.4813,  6.7486,  9.0160, 11.2833], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.1032, grad_fn=<MeanBackward0>)\n",
      "Epoch 34\n",
      "Prediction: tensor([ 4.4968,  6.7562,  9.0155, 11.2749], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0971, grad_fn=<MeanBackward0>)\n",
      "Epoch 35\n",
      "Prediction: tensor([ 4.5119,  6.7635,  9.0151, 11.2666], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0914, grad_fn=<MeanBackward0>)\n",
      "Epoch 36\n",
      "Prediction: tensor([ 4.5265,  6.7706,  9.0146, 11.2587], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "Epoch 37\n",
      "Prediction: tensor([ 4.5407,  6.7774,  9.0142, 11.2509], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "Epoch 38\n",
      "Prediction: tensor([ 4.5544,  6.7841,  9.0138, 11.2434], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "Epoch 39\n",
      "Prediction: tensor([ 4.5678,  6.7905,  9.0133, 11.2361], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "Epoch 40\n",
      "Prediction: tensor([ 4.5807,  6.7968,  9.0129, 11.2291], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "Epoch 41\n",
      "Prediction: tensor([ 4.5933,  6.8029,  9.0126, 11.2222], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0635, grad_fn=<MeanBackward0>)\n",
      "Epoch 42\n",
      "Prediction: tensor([ 4.6054,  6.8088,  9.0122, 11.2156], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "Epoch 43\n",
      "Prediction: tensor([ 4.6172,  6.8145,  9.0118, 11.2091], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0562, grad_fn=<MeanBackward0>)\n",
      "Epoch 44\n",
      "Prediction: tensor([ 4.6287,  6.8201,  9.0115, 11.2028], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0529, grad_fn=<MeanBackward0>)\n",
      "Epoch 45\n",
      "Prediction: tensor([ 4.6398,  6.8255,  9.0111, 11.1968], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0498, grad_fn=<MeanBackward0>)\n",
      "Epoch 46\n",
      "Prediction: tensor([ 4.6506,  6.8307,  9.0108, 11.1909], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0468, grad_fn=<MeanBackward0>)\n",
      "Epoch 47\n",
      "Prediction: tensor([ 4.6611,  6.8358,  9.0105, 11.1852], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0441, grad_fn=<MeanBackward0>)\n",
      "Epoch 48\n",
      "Prediction: tensor([ 4.6712,  6.8407,  9.0101, 11.1796], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0415, grad_fn=<MeanBackward0>)\n",
      "Epoch 49\n",
      "Prediction: tensor([ 4.6811,  6.8454,  9.0098, 11.1742], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0390, grad_fn=<MeanBackward0>)\n",
      "Epoch 50\n",
      "Prediction: tensor([ 4.6906,  6.8501,  9.0095, 11.1690], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0367, grad_fn=<MeanBackward0>)\n",
      "Epoch 51\n",
      "Prediction: tensor([ 4.6999,  6.8546,  9.0093, 11.1640], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0346, grad_fn=<MeanBackward0>)\n",
      "Epoch 52\n",
      "Prediction: tensor([ 4.7089,  6.8589,  9.0090, 11.1591], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0325, grad_fn=<MeanBackward0>)\n",
      "Epoch 53\n",
      "Prediction: tensor([ 4.7176,  6.8631,  9.0087, 11.1543], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0306, grad_fn=<MeanBackward0>)\n",
      "Epoch 54\n",
      "Prediction: tensor([ 4.7260,  6.8672,  9.0085, 11.1497], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0288, grad_fn=<MeanBackward0>)\n",
      "Epoch 55\n",
      "Prediction: tensor([ 4.7342,  6.8712,  9.0082, 11.1452], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0271, grad_fn=<MeanBackward0>)\n",
      "Epoch 56\n",
      "Prediction: tensor([ 4.7422,  6.8751,  9.0080, 11.1408], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0255, grad_fn=<MeanBackward0>)\n",
      "Epoch 57\n",
      "Prediction: tensor([ 4.7499,  6.8788,  9.0077, 11.1366], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0240, grad_fn=<MeanBackward0>)\n",
      "Epoch 58\n",
      "Prediction: tensor([ 4.7574,  6.8824,  9.0075, 11.1325], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0226, grad_fn=<MeanBackward0>)\n",
      "Epoch 59\n",
      "Prediction: tensor([ 4.7647,  6.8860,  9.0073, 11.1286], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0212, grad_fn=<MeanBackward0>)\n",
      "Epoch 60\n",
      "Prediction: tensor([ 4.7717,  6.8894,  9.0070, 11.1247], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0200, grad_fn=<MeanBackward0>)\n",
      "Epoch 61\n",
      "Prediction: tensor([ 4.7785,  6.8927,  9.0068, 11.1210], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0188, grad_fn=<MeanBackward0>)\n",
      "Epoch 62\n",
      "Prediction: tensor([ 4.7852,  6.8959,  9.0066, 11.1174], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0177, grad_fn=<MeanBackward0>)\n",
      "Epoch 63\n",
      "Prediction: tensor([ 4.7916,  6.8990,  9.0064, 11.1139], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0167, grad_fn=<MeanBackward0>)\n",
      "Epoch 64\n",
      "Prediction: tensor([ 4.7978,  6.9020,  9.0062, 11.1104], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0157, grad_fn=<MeanBackward0>)\n",
      "Epoch 65\n",
      "Prediction: tensor([ 4.8039,  6.9050,  9.0061, 11.1071], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0148, grad_fn=<MeanBackward0>)\n",
      "Epoch 66\n",
      "Prediction: tensor([ 4.8098,  6.9078,  9.0059, 11.1039], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0139, grad_fn=<MeanBackward0>)\n",
      "Epoch 67\n",
      "Prediction: tensor([ 4.8155,  6.9106,  9.0057, 11.1008], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0131, grad_fn=<MeanBackward0>)\n",
      "Epoch 68\n",
      "Prediction: tensor([ 4.8210,  6.9133,  9.0055, 11.0978], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0123, grad_fn=<MeanBackward0>)\n",
      "Epoch 69\n",
      "Prediction: tensor([ 4.8263,  6.9158,  9.0054, 11.0949], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0116, grad_fn=<MeanBackward0>)\n",
      "Epoch 70\n",
      "Prediction: tensor([ 4.8315,  6.9184,  9.0052, 11.0920], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0109, grad_fn=<MeanBackward0>)\n",
      "Epoch 71\n",
      "Prediction: tensor([ 4.8366,  6.9208,  9.0050, 11.0893], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0102, grad_fn=<MeanBackward0>)\n",
      "Epoch 72\n",
      "Prediction: tensor([ 4.8415,  6.9232,  9.0049, 11.0866], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0096, grad_fn=<MeanBackward0>)\n",
      "Epoch 73\n",
      "Prediction: tensor([ 4.8462,  6.9255,  9.0047, 11.0840], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0091, grad_fn=<MeanBackward0>)\n",
      "Epoch 74\n",
      "Prediction: tensor([ 4.8508,  6.9277,  9.0046, 11.0815], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0085, grad_fn=<MeanBackward0>)\n",
      "Epoch 75\n",
      "Prediction: tensor([ 4.8553,  6.9299,  9.0045, 11.0791], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0080, grad_fn=<MeanBackward0>)\n",
      "Epoch 76\n",
      "Prediction: tensor([ 4.8596,  6.9320,  9.0043, 11.0767], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0076, grad_fn=<MeanBackward0>)\n",
      "Epoch 77\n",
      "Prediction: tensor([ 4.8638,  6.9340,  9.0042, 11.0744], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0071, grad_fn=<MeanBackward0>)\n",
      "Epoch 78\n",
      "Prediction: tensor([ 4.8679,  6.9360,  9.0041, 11.0722], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0067, grad_fn=<MeanBackward0>)\n",
      "Epoch 79\n",
      "Prediction: tensor([ 4.8719,  6.9379,  9.0040, 11.0700], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0063, grad_fn=<MeanBackward0>)\n",
      "Epoch 80\n",
      "Prediction: tensor([ 4.8757,  6.9398,  9.0038, 11.0679], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0059, grad_fn=<MeanBackward0>)\n",
      "Epoch 81\n",
      "Prediction: tensor([ 4.8794,  6.9416,  9.0037, 11.0659], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0056, grad_fn=<MeanBackward0>)\n",
      "Epoch 82\n",
      "Prediction: tensor([ 4.8830,  6.9433,  9.0036, 11.0639], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0052, grad_fn=<MeanBackward0>)\n",
      "Epoch 83\n",
      "Prediction: tensor([ 4.8865,  6.9450,  9.0035, 11.0620], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0049, grad_fn=<MeanBackward0>)\n",
      "Epoch 84\n",
      "Prediction: tensor([ 4.8899,  6.9467,  9.0034, 11.0601], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0046, grad_fn=<MeanBackward0>)\n",
      "Epoch 85\n",
      "Prediction: tensor([ 4.8932,  6.9483,  9.0033, 11.0583], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0044, grad_fn=<MeanBackward0>)\n",
      "Epoch 86\n",
      "Prediction: tensor([ 4.8964,  6.9498,  9.0032, 11.0566], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0041, grad_fn=<MeanBackward0>)\n",
      "Epoch 87\n",
      "Prediction: tensor([ 4.8995,  6.9513,  9.0031, 11.0549], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0039, grad_fn=<MeanBackward0>)\n",
      "Epoch 88\n",
      "Prediction: tensor([ 4.9025,  6.9528,  9.0030, 11.0532], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0036, grad_fn=<MeanBackward0>)\n",
      "Epoch 89\n",
      "Prediction: tensor([ 4.9054,  6.9542,  9.0029, 11.0517], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0034, grad_fn=<MeanBackward0>)\n",
      "Epoch 90\n",
      "Prediction: tensor([ 4.9083,  6.9556,  9.0028, 11.0501], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0032, grad_fn=<MeanBackward0>)\n",
      "Epoch 91\n",
      "Prediction: tensor([ 4.9110,  6.9569,  9.0027, 11.0486], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0030, grad_fn=<MeanBackward0>)\n",
      "Epoch 92\n",
      "Prediction: tensor([ 4.9137,  6.9582,  9.0027, 11.0472], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0029, grad_fn=<MeanBackward0>)\n",
      "Epoch 93\n",
      "Prediction: tensor([ 4.9163,  6.9594,  9.0026, 11.0457], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0027, grad_fn=<MeanBackward0>)\n",
      "Epoch 94\n",
      "Prediction: tensor([ 4.9188,  6.9606,  9.0025, 11.0444], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0025, grad_fn=<MeanBackward0>)\n",
      "Epoch 95\n",
      "Prediction: tensor([ 4.9212,  6.9618,  9.0024, 11.0430], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0024, grad_fn=<MeanBackward0>)\n",
      "Epoch 96\n",
      "Prediction: tensor([ 4.9236,  6.9630,  9.0024, 11.0418], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0022, grad_fn=<MeanBackward0>)\n",
      "Epoch 97\n",
      "Prediction: tensor([ 4.9259,  6.9641,  9.0023, 11.0405], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0021, grad_fn=<MeanBackward0>)\n",
      "Epoch 98\n",
      "Prediction: tensor([ 4.9281,  6.9651,  9.0022, 11.0393], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0020, grad_fn=<MeanBackward0>)\n",
      "Epoch 99\n",
      "Prediction: tensor([ 4.9302,  6.9662,  9.0022, 11.0381], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0019, grad_fn=<MeanBackward0>)\n",
      "Epoch 100\n",
      "Prediction: tensor([ 4.9323,  6.9672,  9.0021, 11.0370], grad_fn=<AddBackward0>)\n",
      "Loss: tensor(0.0018, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 100 # Converges at alpha = 0.1\n",
    "alpha = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch \" + str(epoch+1))\n",
    "    # Forward pass\n",
    "    Yhat = forward(W, X, b)\n",
    "    print(\"Prediction: \" + str(Yhat))\n",
    "    # Loss prediction\n",
    "    prediction_loss = loss(Y, Yhat)\n",
    "    print(\"Loss: \" + str(prediction_loss))\n",
    "    # Backward pass\n",
    "    # dW, db = gradient(X, Y, Yhat)\n",
    "    prediction_loss.backward()\n",
    "    # Update weights and biases\n",
    "    with torch.no_grad():\n",
    "        W -= alpha * W.grad\n",
    "        b -= alpha * b.grad\n",
    "    # Zero out the gradients\n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

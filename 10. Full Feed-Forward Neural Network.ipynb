{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2887ddc",
   "metadata": {},
   "source": [
    "#### Full Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b06fb2d",
   "metadata": {},
   "source": [
    "A Feed-Forward (Neural) Network (FFN) is essentially a simple neural network where the connections between nodes do not form a cycle. That is to say that there are no backlinks or connection from the $L+1$ layer to the $L$ layer as would be seen in Residual Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864134b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734a2bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration to use GPU if found\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df62a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_size = 784 # Images are 28x28 = 784 pixels each\n",
    "hidden_size = 100 # Number of neurons in the hidden layer\n",
    "num_classes = 10 # 10 classes or digits to be predicted\n",
    "num_epochs = 2 # 2 epochs\n",
    "batch_size = 100 # 100 samples per batch\n",
    "alpha = 1e-3 # Learning rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614450e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST dataset\n",
    "# This dataset contains images of the handwritten numbers 0-9\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=torchvision.transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a19c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize train DataLoader\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64ca7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([1, 4, 3, 7, 4, 7, 2, 5, 3, 4, 1, 4, 8, 4, 2, 7, 2, 1, 0, 6, 1, 9, 2, 0,\n",
      "        7, 2, 1, 9, 3, 2, 3, 4, 7, 3, 4, 5, 2, 5, 4, 3, 6, 9, 3, 9, 8, 8, 9, 6,\n",
      "        3, 1, 0, 1, 9, 9, 0, 2, 8, 2, 9, 0, 9, 4, 7, 6, 0, 3, 0, 0, 1, 0, 9, 1,\n",
      "        1, 7, 7, 4, 4, 5, 9, 0, 4, 1, 9, 4, 3, 3, 9, 2, 3, 6, 5, 6, 9, 2, 9, 2,\n",
      "        2, 5, 1, 5])]\n"
     ]
    }
   ],
   "source": [
    "# Check one sample\n",
    "sample = iter(train_dataloader).next()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b895b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = sample[1]\n",
    "# # sample = sample.view(sample.shape[0], 28*28)\n",
    "# print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a781e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the first few samples in images\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(sample[0][i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "123756da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FFN\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FFN, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (self.l2(self.relu(self.l1(x))))\n",
    "        # We are not using any softmax layer or one-hot encoding the output since we are going to be using CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e12f883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = FFN(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes).to(device) # The model has to be pushed to the device as well\n",
    "\n",
    "# Define criterion\n",
    "criterion = nn.CrossEntropyLoss() # This is a multiclass classification problem\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=alpha) # The Adam optimizer will be used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71f73760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2, step: 100/600, loss: 0.389\n",
      "Epoch: 1/2, step: 200/600, loss: 0.350\n",
      "Epoch: 1/2, step: 300/600, loss: 0.240\n",
      "Epoch: 1/2, step: 400/600, loss: 0.269\n",
      "Epoch: 1/2, step: 500/600, loss: 0.144\n",
      "Epoch: 1/2, step: 600/600, loss: 0.150\n",
      "Epoch: 2/2, step: 100/600, loss: 0.172\n",
      "Epoch: 2/2, step: 200/600, loss: 0.294\n",
      "Epoch: 2/2, step: 300/600, loss: 0.219\n",
      "Epoch: 2/2, step: 400/600, loss: 0.163\n",
      "Epoch: 2/2, step: 500/600, loss: 0.154\n",
      "Epoch: 2/2, step: 600/600, loss: 0.176\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_iters = len(train_dataloader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        # images are of the shape 100, 1, 28, 28\n",
    "        # There are 100 images per batch\n",
    "        # Each image has 1 channel (grayscale) and 28x28 pixels\n",
    "        # The required format is 100, 784\n",
    "        # 100 samples each with 784 features (pixel values)\n",
    "        images = images.view(images.shape[0], 28*28).to(device) # Reshape the tensor and push it to device\n",
    "        labels = labels.to(device) # Also push labels to device\n",
    "        \n",
    "        # Forward propagation\n",
    "        predicted_output = model(images)\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(predicted_output, labels)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Parameter update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Print some information\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch: {epoch+1}/{num_epochs}, step: {i+1}/{num_iters}, loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e70e5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.570\n"
     ]
    }
   ],
   "source": [
    "# Testing and evaluation\n",
    "with torch.no_grad():\n",
    "    samples = 0\n",
    "    correct = 0\n",
    "    for i, (images, labels) in enumerate(test_dataloader):\n",
    "        # images are of the shape 100, 1, 28, 28\n",
    "        # There are 100 images per batch\n",
    "        # Each image has 1 channel (grayscale) and 28x28 pixels\n",
    "        # The required format is 100, 784\n",
    "        # 100 samples each with 784 features (pixel values)\n",
    "        images = images.view(images.shape[0], 28*28).to(device) # Reshape the tensor and push it to device\n",
    "        labels = labels.to(device) # Also push labels to device\n",
    "        \n",
    "        # Forward propagation\n",
    "        predicted_output = model(images)\n",
    "        _, predicted_output = torch.max(predicted_output, 1) # Acquire the class label\n",
    "        \n",
    "        # Overall loss\n",
    "        samples += labels.shape[0] # Number of labelled samples received\n",
    "        correct += (predicted_output==labels).sum().item() # Number of correctly predicted samples\n",
    "    \n",
    "    acc = correct / samples * 100.0\n",
    "    print(f\"Accuracy: {acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
